{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习\n",
    "\n",
    "### 1.1 概念\n",
    "\n",
    "reward函数，什么时候做的算是更好\n",
    "\n",
    "discount factor $\\gamma$ ， 一般取$\\gamma = 0.5 $  $R_0$为当前状态不动的回报函数\n",
    "\n",
    "$$\n",
    "Return =R_0+ \\gamma R_1+ \\gamma ^2 R_2 +\\gamma ^3 R_3+...\n",
    "$$\n",
    "Markov Decision Process马尔可夫决策过程， 回报只取决于当前的状态\n",
    "\n",
    "$Q(s,a)$, 状态$s$ (一般来说，s是一个多元函数),  动作$a$，下的reward\n",
    "\n",
    "$^*Q(s,a)=\\underset{a}{\\mathop{\\max }}Q(s,a)$  \n",
    "\n",
    "如果$a'$表示下一个状态的动作，$s'$表示下一个状态\n",
    "\n",
    "对回报函数再进行一定的改写，改为：\n",
    "$$\n",
    "Return =R_0+ \\gamma (R_1+ \\gamma  R_2 +\\gamma ^2 R_3+...)\n",
    "$$\n",
    "可以得到贝尔曼方程\n",
    "$$\n",
    "Q(s,a)=R(s) + \\gamma \\underset{a'}{\\mathop{\\max }}Q(s',a')\n",
    "$$\n",
    "stochastic随机情况下，像火车探测车有一定几率不会沿着预定的方向走（比如说它可能滑坡了，风向 etc），此时我们需要把贝尔曼方程改写为\n",
    "$$\n",
    "Q(s,a)=R(s) + \\gamma E(\\underset{a'}{\\mathop{\\max }}Q(s',a'))\n",
    "$$\n",
    "\n",
    "### 1.2 搭建神经网络\n",
    "\n",
    "$x$为$(s,a),   $y为$R(s)+ \\gamma \\underset{a'}{\\mathop{\\max }}Q(s',a')$,   要使得$f_{w,B}(x)\\approx y$\n",
    "\n",
    "**Repeat{**\n",
    "$$\n",
    "初始化(s,aR(s),s') \\\\\n",
    "Replay  Buffer存储 10，000个附近可能的 (s,aR(s),s')\\\\\n",
    "训练神经网络 使得f_{w,B}(x) \\approx  y\n",
    "$$\n",
    "**}**\n",
    "\n",
    "### 1.3 算法改进\n",
    "\n",
    "###### 1.同时输出比训练多个更有效\n",
    "\n",
    "   ![](images/p062.png)\n",
    "\n",
    "##### 2. $\\epsilon $ greedy\n",
    "     （exploit)$1-\\epsilon$的概率贪婪使得$maxQ(s',a')$  ,  (explore)  $\\epsilon$的概率是随机走一步。  可以避免某些极端的情况，比如某一步reward很小，但是之后会非常大\n",
    "\n",
    "##### 3.mini-batch 小批量 \n",
    "\n",
    "当数据量很大的时候时候，每次都从大样本里面抽取不同的小样本来训练 。噪声多，但是训练成本会有效很多\n",
    "\n",
    "![](images/p061.png)\n",
    "##### 随机梯度下降\n",
    "如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。\n",
    "\n",
    "在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：\n",
    "\n",
    "​                                                           $$cost\\left(  \\theta, \\left( {x}^{(i)} , {y}^{(i)} \\right)  \\right) = \\frac{1}{2}\\left( {h}_{\\theta}\\left({x}^{(i)}\\right)-{y}^{{(i)}} \\right)^{2}$$\n",
    "\n",
    "\n",
    "\n",
    "随机梯度下降算法在每一次计算之后便更新参数 ${{\\theta }}$ ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊\n",
    "##### 小批量梯度下降算法\n",
    "介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数$b$次训练实例，便更新一次参数  ${{\\theta }}$ 。 **Repeat** {\n",
    "\n",
    " **for** $i = 1:m${\n",
    "\n",
    " ​       $\\theta:={\\theta}_{j}-\\alpha\\frac{1}{b}\\sum_\\limits{k=i}^{i+b-1}\\left( {h}_{\\theta}\\left({x}^{(k)}\\right)-{y}^{(k)} \\right){{x}_{j}}^{(k)}$      \n",
    "\n",
    "​       (**for** $j=0:n$)\n",
    "\n",
    "​      $ i +=10 $   \n",
    "\n",
    " ​     }\n",
    " }\n",
    "\n",
    "通常我们会令 $b$ 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 $b$个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。\n",
    "\n",
    "##### 4. and soft update  软更新\n",
    "不是一次性更新，防止震荡和一次性转向\n",
    "\n",
    "   $Q=Q_{new}$ 变为$Q=0.01Q_{new}+0.99Q$,  每次只更新一点点"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6246b25e200e4c5124e3e61789ac81350562f0761bbcf92ad9e48654207659c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
