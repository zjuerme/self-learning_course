{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzk7iX_CodX6",
    "tags": []
   },
   "source": [
    "# 基于内容的推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Xu-w_RmNwCV5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 01:51:22.397943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-11 01:51:22.567785: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-03-11 01:51:22.571086: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib\n",
      "2023-03-11 01:51:22.571098: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-11 01:51:23.220277: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib\n",
      "2023-03-11 01:51:23.220787: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib\n",
      "2023-03-11 01:51:23.220825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "from numpy import genfromtxt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tabulate\n",
    "from recsysNN_utils import *\n",
    "pd.set_option(\"display.precision\", 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "### 2.1 Content-based filtering with a neural network\n",
    "\n",
    "In the collaborative filtering lab, you generated two vectors, a user vector and an item/movie vector whose dot product would predict a rating. The vectors were derived solely from the ratings.   \n",
    "\n",
    "Content-based filtering also generates a user and movie feature vector but recognizes there may be other information available about the user and/or movie that may improve the prediction. The additional information is provided to a neural network which then generates the user and movie vector as shown below.\n",
    "<figure>\n",
    "    <center> <img src=\"./images/RecSysNN.png\"   style=\"width:500px;height:280px;\" ></center>\n",
    "</figure>\n",
    "The movie content provided to the network is a combination of the original data and some 'engineered features'. Recall the feature engineering discussion and lab from Course 1, Week 2, lab 4. The original features are the year the movie was released and the movie's genre presented as a one-hot vector. There are 14 genres. The engineered feature is an average rating derived from the user ratings. Movies with multiple genre have a training vector per genre. \n",
    "\n",
    "The user content is composed of only engineered features. A per genre average rating is computed per user. Additionally, a user id, rating count and rating average are available, but are not included in the training or prediction content. They are useful in interpreting data.\n",
    "\n",
    "The training set consists of all the ratings made by the users in the data set. The user and movie/item vectors are presented to the above network together as a training set. The user vector is the same for all the movies rated by the user. \n",
    "\n",
    "Below, let's load and display some of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "M5gfMLYgxCD1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training vectors: 58187\n"
     ]
    }
   ],
   "source": [
    "# Load Data, set configuration variables\n",
    "item_train, user_train, y_train, item_features, user_features, item_vecs, movie_dict, user_to_genre = load_data()\n",
    "\n",
    "num_user_features = user_train.shape[1] - 3  # remove userid, rating count and ave rating during training\n",
    "num_item_features = item_train.shape[1] - 1  # remove movie id at train time\n",
    "uvs = 3  # user genre vector start\n",
    "ivs = 3  # item genre vector start\n",
    "u_s = 3  # start of columns to use in training, user\n",
    "i_s = 1  # start of columns to use in training, items\n",
    "scaledata = True  # applies the standard scalar to data if true\n",
    "print(f\"Number of training vectors: {len(item_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the user and item/movie features are not used in training. Below, the features in brackets \"[]\" such as the \"user id\", \"rating count\" and \"rating ave\" are not included when the model is trained and used. Note, the user vector is the same for all the movies rated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "### 2.2 Preparing the training data\n",
    "Recall in Course 1, Week 2, you explored feature scaling as a means of improving convergence. We'll scale the input features using the [scikit learn StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). This was used in Course 1, Week 2, Lab 5.  Below, the inverse_transform is also shown to produce the original inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# scale training data\n",
    "if scaledata:\n",
    "    item_train_save = item_train\n",
    "    user_train_save = user_train\n",
    "\n",
    "    scalerItem = StandardScaler()\n",
    "    scalerItem.fit(item_train)\n",
    "    item_train = scalerItem.transform(item_train)\n",
    "\n",
    "    scalerUser = StandardScaler()\n",
    "    scalerUser.fit(user_train)\n",
    "    user_train = scalerUser.transform(user_train)\n",
    "\n",
    "    print(np.allclose(item_train_save, scalerItem.inverse_transform(item_train)))\n",
    "    print(np.allclose(user_train_save, scalerUser.inverse_transform(user_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow us to evaluate the results, we will split the data into training and test sets as was discussed in Course 2, Week 3. Here we will use [sklean train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split and shuffle the data. Note that setting the initial random state to the same value ensures item, user, and y are shuffled identically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie/item training data shape: (46549, 17)\n",
      "movie/item test  data shape: (11638, 17)\n"
     ]
    }
   ],
   "source": [
    "item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
    "print(f\"movie/item training data shape: {item_train.shape}\")\n",
    "print(f\"movie/item test  data shape: {item_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled, shuffled data now has a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: center;\"> [user id] </th><th style=\"text-align: center;\"> [rating count] </th><th style=\"text-align: center;\"> [rating ave] </th><th style=\"text-align: center;\"> Act ion </th><th style=\"text-align: center;\"> Adve nture </th><th style=\"text-align: center;\"> Anim ation </th><th style=\"text-align: center;\"> Chil dren </th><th style=\"text-align: center;\"> Com edy </th><th style=\"text-align: center;\"> Crime </th><th style=\"text-align: center;\"> Docum entary </th><th style=\"text-align: center;\"> Drama </th><th style=\"text-align: center;\"> Fan tasy </th><th style=\"text-align: center;\"> Hor ror </th><th style=\"text-align: center;\"> Mys tery </th><th style=\"text-align: center;\"> Rom ance </th><th style=\"text-align: center;\"> Sci -Fi </th><th style=\"text-align: center;\"> Thri ller </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: center;\">     1     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     0.6      </td><td style=\"text-align: center;\">   0.7   </td><td style=\"text-align: center;\">    0.6     </td><td style=\"text-align: center;\">    0.6     </td><td style=\"text-align: center;\">    0.7    </td><td style=\"text-align: center;\">   0.7   </td><td style=\"text-align: center;\">  0.5  </td><td style=\"text-align: center;\">     0.7      </td><td style=\"text-align: center;\">  0.2  </td><td style=\"text-align: center;\">   0.3    </td><td style=\"text-align: center;\">   0.3   </td><td style=\"text-align: center;\">   0.5    </td><td style=\"text-align: center;\">   0.5    </td><td style=\"text-align: center;\">   0.8   </td><td style=\"text-align: center;\">    0.5    </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     1.6      </td><td style=\"text-align: center;\">   1.5   </td><td style=\"text-align: center;\">    1.7     </td><td style=\"text-align: center;\">    0.9     </td><td style=\"text-align: center;\">    1.0    </td><td style=\"text-align: center;\">   1.4   </td><td style=\"text-align: center;\">  0.8  </td><td style=\"text-align: center;\">     -1.2     </td><td style=\"text-align: center;\">  1.2  </td><td style=\"text-align: center;\">   1.2    </td><td style=\"text-align: center;\">   1.6   </td><td style=\"text-align: center;\">   0.9    </td><td style=\"text-align: center;\">   1.4    </td><td style=\"text-align: center;\">   1.2   </td><td style=\"text-align: center;\">    1.0    </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     0.8      </td><td style=\"text-align: center;\">   0.6   </td><td style=\"text-align: center;\">    0.7     </td><td style=\"text-align: center;\">    0.5     </td><td style=\"text-align: center;\">    0.6    </td><td style=\"text-align: center;\">   0.6   </td><td style=\"text-align: center;\">  0.3  </td><td style=\"text-align: center;\">     -1.2     </td><td style=\"text-align: center;\">  0.7  </td><td style=\"text-align: center;\">   0.8    </td><td style=\"text-align: center;\">   0.9   </td><td style=\"text-align: center;\">   0.6    </td><td style=\"text-align: center;\">   0.2    </td><td style=\"text-align: center;\">   0.6   </td><td style=\"text-align: center;\">    0.6    </td></tr>\n",
       "<tr><td style=\"text-align: center;\">     1     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     -0.1     </td><td style=\"text-align: center;\">   0.2   </td><td style=\"text-align: center;\">    -0.1    </td><td style=\"text-align: center;\">    0.3     </td><td style=\"text-align: center;\">    0.7    </td><td style=\"text-align: center;\">   0.3   </td><td style=\"text-align: center;\">  0.2  </td><td style=\"text-align: center;\">     1.0      </td><td style=\"text-align: center;\"> -0.5  </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">  -2.1   </td><td style=\"text-align: center;\">   0.5    </td><td style=\"text-align: center;\">   0.7    </td><td style=\"text-align: center;\">   0.3   </td><td style=\"text-align: center;\">    0.0    </td></tr>\n",
       "<tr><td style=\"text-align: center;\">    -1     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     -1.3     </td><td style=\"text-align: center;\">  -0.8   </td><td style=\"text-align: center;\">    -0.8    </td><td style=\"text-align: center;\">    0.1     </td><td style=\"text-align: center;\">   -0.1    </td><td style=\"text-align: center;\">  -1.1   </td><td style=\"text-align: center;\"> -0.9  </td><td style=\"text-align: center;\">     -1.2     </td><td style=\"text-align: center;\"> -1.5  </td><td style=\"text-align: center;\">   -0.6   </td><td style=\"text-align: center;\">  -0.5   </td><td style=\"text-align: center;\">   -0.6   </td><td style=\"text-align: center;\">   -0.9   </td><td style=\"text-align: center;\">  -0.4   </td><td style=\"text-align: center;\">   -0.9    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: center;\"> [user id] </th><th style=\"text-align: center;\"> [rating count] </th><th style=\"text-align: center;\"> [rating ave] </th><th style=\"text-align: center;\"> Act ion </th><th style=\"text-align: center;\"> Adve nture </th><th style=\"text-align: center;\"> Anim ation </th><th style=\"text-align: center;\"> Chil dren </th><th style=\"text-align: center;\"> Com edy </th><th style=\"text-align: center;\"> Crime </th><th style=\"text-align: center;\"> Docum entary </th><th style=\"text-align: center;\"> Drama </th><th style=\"text-align: center;\"> Fan tasy </th><th style=\"text-align: center;\"> Hor ror </th><th style=\"text-align: center;\"> Mys tery </th><th style=\"text-align: center;\"> Rom ance </th><th style=\"text-align: center;\"> Sci -Fi </th><th style=\"text-align: center;\"> Thri ller </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: center;\">     1     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     0.6      </td><td style=\"text-align: center;\">   0.7   </td><td style=\"text-align: center;\">    0.6     </td><td style=\"text-align: center;\">    0.6     </td><td style=\"text-align: center;\">    0.7    </td><td style=\"text-align: center;\">   0.7   </td><td style=\"text-align: center;\">  0.5  </td><td style=\"text-align: center;\">     0.7      </td><td style=\"text-align: center;\">  0.2  </td><td style=\"text-align: center;\">   0.3    </td><td style=\"text-align: center;\">   0.3   </td><td style=\"text-align: center;\">   0.5    </td><td style=\"text-align: center;\">   0.5    </td><td style=\"text-align: center;\">   0.8   </td><td style=\"text-align: center;\">    0.5    </td></tr>\\n<tr><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     1.6      </td><td style=\"text-align: center;\">   1.5   </td><td style=\"text-align: center;\">    1.7     </td><td style=\"text-align: center;\">    0.9     </td><td style=\"text-align: center;\">    1.0    </td><td style=\"text-align: center;\">   1.4   </td><td style=\"text-align: center;\">  0.8  </td><td style=\"text-align: center;\">     -1.2     </td><td style=\"text-align: center;\">  1.2  </td><td style=\"text-align: center;\">   1.2    </td><td style=\"text-align: center;\">   1.6   </td><td style=\"text-align: center;\">   0.9    </td><td style=\"text-align: center;\">   1.4    </td><td style=\"text-align: center;\">   1.2   </td><td style=\"text-align: center;\">    1.0    </td></tr>\\n<tr><td style=\"text-align: center;\">     0     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     0.8      </td><td style=\"text-align: center;\">   0.6   </td><td style=\"text-align: center;\">    0.7     </td><td style=\"text-align: center;\">    0.5     </td><td style=\"text-align: center;\">    0.6    </td><td style=\"text-align: center;\">   0.6   </td><td style=\"text-align: center;\">  0.3  </td><td style=\"text-align: center;\">     -1.2     </td><td style=\"text-align: center;\">  0.7  </td><td style=\"text-align: center;\">   0.8    </td><td style=\"text-align: center;\">   0.9   </td><td style=\"text-align: center;\">   0.6    </td><td style=\"text-align: center;\">   0.2    </td><td style=\"text-align: center;\">   0.6   </td><td style=\"text-align: center;\">    0.6    </td></tr>\\n<tr><td style=\"text-align: center;\">     1     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     -0.1     </td><td style=\"text-align: center;\">   0.2   </td><td style=\"text-align: center;\">    -0.1    </td><td style=\"text-align: center;\">    0.3     </td><td style=\"text-align: center;\">    0.7    </td><td style=\"text-align: center;\">   0.3   </td><td style=\"text-align: center;\">  0.2  </td><td style=\"text-align: center;\">     1.0      </td><td style=\"text-align: center;\"> -0.5  </td><td style=\"text-align: center;\">   -0.7   </td><td style=\"text-align: center;\">  -2.1   </td><td style=\"text-align: center;\">   0.5    </td><td style=\"text-align: center;\">   0.7    </td><td style=\"text-align: center;\">   0.3   </td><td style=\"text-align: center;\">    0.0    </td></tr>\\n<tr><td style=\"text-align: center;\">    -1     </td><td style=\"text-align: center;\">       0        </td><td style=\"text-align: center;\">     -1.3     </td><td style=\"text-align: center;\">  -0.8   </td><td style=\"text-align: center;\">    -0.8    </td><td style=\"text-align: center;\">    0.1     </td><td style=\"text-align: center;\">   -0.1    </td><td style=\"text-align: center;\">  -1.1   </td><td style=\"text-align: center;\"> -0.9  </td><td style=\"text-align: center;\">     -1.2     </td><td style=\"text-align: center;\"> -1.5  </td><td style=\"text-align: center;\">   -0.6   </td><td style=\"text-align: center;\">  -0.5   </td><td style=\"text-align: center;\">   -0.6   </td><td style=\"text-align: center;\">   -0.9   </td><td style=\"text-align: center;\">  -0.4   </td><td style=\"text-align: center;\">   -0.9    </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pprint_train(user_train, user_features, uvs, u_s, maxcount=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KeoAhs95LRop"
   },
   "source": [
    "Scale the target ratings using a Min Max Scaler to scale the target to be between -1 and 1. We use scikit-learn because it has an inverse_transform. [scikit learn MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "A8myXMxFC8lP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46549, 1) (11638, 1)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler((-1, 1))\n",
    "scaler.fit(y_train.reshape(-1, 1))\n",
    "ynorm_train = scaler.transform(y_train.reshape(-1, 1))\n",
    "ynorm_test = scaler.transform(y_test.reshape(-1, 1))\n",
    "print(ynorm_train.shape, ynorm_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3\"></a>\n",
    "## 3 - Neural Network for content-based filtering\n",
    "Now, let's construct a neural network as described in the figure above. It will have two networks that are combined by a dot product. You will construct the two networks. In this example, they will be identical. Note that these networks do not need to be the same. If the user content was substantially larger than the movie content, you might elect to increase the complexity of the user network relative to the movie network. In this case, the content is similar, so the networks are the same.\n",
    "\n",
    "- Use a Keras sequential model\n",
    "    - The first layer is a dense layer with 256 units and a relu activation.\n",
    "    - The second layer is a dense layer with 128 units and a relu activation.\n",
    "    - The third layer is a dense layer with `num_outputs` units and a linear or no activation.   \n",
    "    \n",
    "The remainder of the network will be provided. The provided code does not use the Keras sequential model but instead uses the Keras [functional api](https://keras.io/guides/functional_api/). This format allows for more flexibility in how components are interconnected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "CBjZ2HhRwpa0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 14)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 16)]         0           []                               \n",
      "                                                                                                  \n",
      " sequential (Sequential)        (None, 32)           40864       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)      (None, 32)           41376       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize (TFOpLamb  (None, 32)          0           ['sequential[0][0]']             \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.math.l2_normalize_1 (TFOpLa  (None, 32)          0           ['sequential_1[0][0]']           \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1)            0           ['tf.math.l2_normalize[0][0]',   \n",
      "                                                                  'tf.math.l2_normalize_1[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 82,240\n",
      "Trainable params: 82,240\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-11 01:51:25.581536: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/ros/melodic/lib\n",
      "2023-03-11 01:51:25.581595: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-11 01:51:25.581613: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ubuntu): /proc/driver/nvidia/version does not exist\n",
      "2023-03-11 01:51:25.581849: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# GRADED_CELL\n",
    "# UNQ_C1\n",
    "\n",
    "num_outputs = 32\n",
    "tf.random.set_seed(1)\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ###   \n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs),\n",
    "    ### END CODE HERE ###  \n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ###     \n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs),\n",
    "    ### END CODE HERE ###  \n",
    "])\n",
    "\n",
    "# create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.l2_normalize(vu, axis=1) # 标准化1，提升性能\n",
    "\n",
    "# create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
    "\n",
    "# compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])  #vu,vm做点乘\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use a mean squared error loss and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "pGK5MEUowxN4"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)  #用Adam算法来自动选择学习率\n",
    "model.compile(optimizer=opt, loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "6zHf7eASw0tN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1455/1455 [==============================] - 7s 4ms/step - loss: 0.1249\n",
      "Epoch 2/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1182\n",
      "Epoch 3/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1167\n",
      "Epoch 4/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1151\n",
      "Epoch 5/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1140\n",
      "Epoch 6/30\n",
      "1455/1455 [==============================] - 7s 5ms/step - loss: 0.1125\n",
      "Epoch 7/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1118\n",
      "Epoch 8/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1111\n",
      "Epoch 9/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1102\n",
      "Epoch 10/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1096\n",
      "Epoch 11/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1086\n",
      "Epoch 12/30\n",
      "1455/1455 [==============================] - 5s 3ms/step - loss: 0.1077\n",
      "Epoch 13/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1063\n",
      "Epoch 14/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1053\n",
      "Epoch 15/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1046\n",
      "Epoch 16/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1041\n",
      "Epoch 17/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1034\n",
      "Epoch 18/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1032\n",
      "Epoch 19/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1025\n",
      "Epoch 20/30\n",
      "1455/1455 [==============================] - 5s 3ms/step - loss: 0.1021\n",
      "Epoch 21/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1015\n",
      "Epoch 22/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1011\n",
      "Epoch 23/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1006\n",
      "Epoch 24/30\n",
      "1455/1455 [==============================] - 6s 4ms/step - loss: 0.1003\n",
      "Epoch 25/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.1000\n",
      "Epoch 26/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.0995\n",
      "Epoch 27/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.0991\n",
      "Epoch 28/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.0990\n",
      "Epoch 29/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.0985\n",
      "Epoch 30/30\n",
      "1455/1455 [==============================] - 5s 4ms/step - loss: 0.0982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f639568d580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(1)\n",
    "model.fit([user_train[:, u_s:], item_train[:, i_s:]], ynorm_train, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model to determine loss on the test data. It is comparable to the training loss indicating the model has not substantially overfit the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364/364 [==============================] - 1s 2ms/step - loss: 0.1049\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10493991523981094"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], ynorm_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xsre-gquwEls"
   },
   "source": [
    "<a name=\"3.1\"></a>\n",
    "### 3.1 Predictions\n",
    "Below, you'll use your model to make predictions in a number of circumstances. \n",
    "#### Predictions for a new user\n",
    "First, we'll create a new user and have the model suggest movies for that user. After you have tried this example on the example user content, feel free to change the user content to match your own preferences and see what the model suggests. Note that ratings are between 0.5 and 5.0, inclusive, in half-step increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4_7nZyPiVJ4r"
   },
   "outputs": [],
   "source": [
    "new_user_id = 5000\n",
    "new_rating_ave = 1.0\n",
    "new_action = 1.0\n",
    "new_adventure = 1\n",
    "new_animation = 1\n",
    "new_childrens = 1\n",
    "new_comedy = 5\n",
    "new_crime = 1\n",
    "new_documentary = 1\n",
    "new_drama = 1\n",
    "new_fantasy = 1\n",
    "new_horror = 1\n",
    "new_mystery = 1\n",
    "new_romance = 5\n",
    "new_scifi = 5\n",
    "new_thriller = 1\n",
    "new_rating_count = 3\n",
    "\n",
    "user_vec = np.array([[new_user_id, new_rating_count, new_rating_ave,\n",
    "                    new_action, new_adventure, new_animation, new_childrens,\n",
    "                    new_comedy, new_crime, new_documentary,\n",
    "                    new_drama, new_fantasy, new_horror, new_mystery,\n",
    "                    new_romance, new_scifi, new_thriller]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's look at the top-rated movies for the new user. Recall, the user vector had genres that favored Comedy and Romance.\n",
    "Below, we'll use a set of movie/item vectors, `item_vecs` that have a vector for each movie in the training/test set. This is matched with the user vector above and the scaled vectors are used to predict ratings for all the movies for our new user above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">    y_p</th><th style=\"text-align: right;\">  movie id</th><th style=\"text-align: right;\">  rating ave</th><th>title                       </th><th>genres                         </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">4.80556</td><td style=\"text-align: right;\">      5135</td><td style=\"text-align: right;\">     3.83333</td><td>Monsoon Wedding (2001)      </td><td>Comedy|Romance                 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">4.79444</td><td style=\"text-align: right;\">      5791</td><td style=\"text-align: right;\">     3.78125</td><td>Frida (2002)                </td><td>Drama|Romance                  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">4.7929 </td><td style=\"text-align: right;\">      4246</td><td style=\"text-align: right;\">     3.62308</td><td>Bridget Jones&#x27;s Diary (2001)</td><td>Comedy|Drama|Romance           </td></tr>\n",
       "<tr><td style=\"text-align: right;\">4.78997</td><td style=\"text-align: right;\">      5377</td><td style=\"text-align: right;\">     3.71591</td><td>About a Boy (2002)          </td><td>Comedy|Drama|Romance           </td></tr>\n",
       "<tr><td style=\"text-align: right;\">4.78541</td><td style=\"text-align: right;\">      5992</td><td style=\"text-align: right;\">     3.7    </td><td>Hours, The (2002)           </td><td>Drama|Romance                  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">4.7818 </td><td style=\"text-align: right;\">      4975</td><td style=\"text-align: right;\">     3.42045</td><td>Vanilla Sky (2001)          </td><td>Mystery|Romance|Sci-Fi|Thriller</td></tr>\n",
       "<tr><td style=\"text-align: right;\">4.7785 </td><td style=\"text-align: right;\">      5110</td><td style=\"text-align: right;\">     3.60417</td><td>Super Troopers (2001)       </td><td>Comedy|Crime|Mystery           </td></tr>\n",
       "<tr><td style=\"text-align: right;\">4.775  </td><td style=\"text-align: right;\">      4816</td><td style=\"text-align: right;\">     3.50926</td><td>Zoolander (2001)            </td><td>Comedy                         </td></tr>\n",
       "<tr><td style=\"text-align: right;\">4.77263</td><td style=\"text-align: right;\">      4641</td><td style=\"text-align: right;\">     3.5    </td><td>Ghost World (2001)          </td><td>Comedy|Drama                   </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: right;\">    y_p</th><th style=\"text-align: right;\">  movie id</th><th style=\"text-align: right;\">  rating ave</th><th>title                       </th><th>genres                         </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: right;\">4.80556</td><td style=\"text-align: right;\">      5135</td><td style=\"text-align: right;\">     3.83333</td><td>Monsoon Wedding (2001)      </td><td>Comedy|Romance                 </td></tr>\\n<tr><td style=\"text-align: right;\">4.79444</td><td style=\"text-align: right;\">      5791</td><td style=\"text-align: right;\">     3.78125</td><td>Frida (2002)                </td><td>Drama|Romance                  </td></tr>\\n<tr><td style=\"text-align: right;\">4.7929 </td><td style=\"text-align: right;\">      4246</td><td style=\"text-align: right;\">     3.62308</td><td>Bridget Jones&#x27;s Diary (2001)</td><td>Comedy|Drama|Romance           </td></tr>\\n<tr><td style=\"text-align: right;\">4.78997</td><td style=\"text-align: right;\">      5377</td><td style=\"text-align: right;\">     3.71591</td><td>About a Boy (2002)          </td><td>Comedy|Drama|Romance           </td></tr>\\n<tr><td style=\"text-align: right;\">4.78541</td><td style=\"text-align: right;\">      5992</td><td style=\"text-align: right;\">     3.7    </td><td>Hours, The (2002)           </td><td>Drama|Romance                  </td></tr>\\n<tr><td style=\"text-align: right;\">4.7818 </td><td style=\"text-align: right;\">      4975</td><td style=\"text-align: right;\">     3.42045</td><td>Vanilla Sky (2001)          </td><td>Mystery|Romance|Sci-Fi|Thriller</td></tr>\\n<tr><td style=\"text-align: right;\">4.7785 </td><td style=\"text-align: right;\">      5110</td><td style=\"text-align: right;\">     3.60417</td><td>Super Troopers (2001)       </td><td>Comedy|Crime|Mystery           </td></tr>\\n<tr><td style=\"text-align: right;\">4.775  </td><td style=\"text-align: right;\">      4816</td><td style=\"text-align: right;\">     3.50926</td><td>Zoolander (2001)            </td><td>Comedy                         </td></tr>\\n<tr><td style=\"text-align: right;\">4.77263</td><td style=\"text-align: right;\">      4641</td><td style=\"text-align: right;\">     3.5    </td><td>Ghost World (2001)          </td><td>Comedy|Drama                   </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate and replicate the user vector to match the number movies in the data set.\n",
    "user_vecs = gen_user_vecs(user_vec,len(item_vecs))\n",
    "\n",
    "# scale the vectors and make predictions for all movies. Return results sorted by rating.\n",
    "sorted_index, sorted_ypu, sorted_items, sorted_user = predict_uservec(user_vecs,  item_vecs, model, u_s, i_s, \n",
    "                                                                       scaler, scalerUser, scalerItem, scaledata=scaledata)\n",
    "\n",
    "print_pred_movies(sorted_ypu, sorted_user, sorted_items, movie_dict, maxcount = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do create a user above, it is worth noting that the network was trained to predict a user rating given a user vector that includes a **set** of user genre ratings.  Simply providing a maximum rating for a single genre and minimum ratings for the rest may not be meaningful to the network if there were no users with similar sets of ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions for an existing user.\n",
    "Let's look at the predictions for \"user 36\", one of the users in the data set. We can compare the predicted ratings with the model's ratings. Note that movies with multiple genre's show up multiple times in the training data. For example,'The Time Machine' has three genre's: Adventure, Action, Sci-Fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  y_p</th><th style=\"text-align: right;\">  y</th><th style=\"text-align: right;\">  user</th><th style=\"text-align: right;\">  user genre ave</th><th style=\"text-align: right;\">  movie rating ave</th><th>title                   </th><th>genres   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">  3.0</td><td style=\"text-align: right;\">3.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            3.00</td><td style=\"text-align: right;\">              2.86</td><td>Time Machine, The (2002)</td><td>Adventure</td></tr>\n",
       "<tr><td style=\"text-align: right;\">  2.9</td><td style=\"text-align: right;\">3.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            3.00</td><td style=\"text-align: right;\">              2.86</td><td>Time Machine, The (2002)</td><td>Action   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  2.8</td><td style=\"text-align: right;\">3.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            3.00</td><td style=\"text-align: right;\">              2.86</td><td>Time Machine, The (2002)</td><td>Sci-Fi   </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  2.0</td><td style=\"text-align: right;\">1.5</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.75</td><td style=\"text-align: right;\">              3.52</td><td>Road to Perdition (2002)</td><td>Crime    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  2.0</td><td style=\"text-align: right;\">2.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.75</td><td style=\"text-align: right;\">              3.52</td><td>Gangs of New York (2002)</td><td>Crime    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  1.9</td><td style=\"text-align: right;\">1.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.50</td><td style=\"text-align: right;\">              4.00</td><td>Beautiful Mind, A (2001)</td><td>Drama    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  1.8</td><td style=\"text-align: right;\">1.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.00</td><td style=\"text-align: right;\">              4.00</td><td>Beautiful Mind, A (2001)</td><td>Romance  </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  1.6</td><td style=\"text-align: right;\">2.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.50</td><td style=\"text-align: right;\">              3.52</td><td>Gangs of New York (2002)</td><td>Drama    </td></tr>\n",
       "<tr><td style=\"text-align: right;\">  1.6</td><td style=\"text-align: right;\">1.5</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.50</td><td style=\"text-align: right;\">              3.52</td><td>Road to Perdition (2002)</td><td>Drama    </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th style=\"text-align: right;\">  y_p</th><th style=\"text-align: right;\">  y</th><th style=\"text-align: right;\">  user</th><th style=\"text-align: right;\">  user genre ave</th><th style=\"text-align: right;\">  movie rating ave</th><th>title                   </th><th>genres   </th></tr>\\n</thead>\\n<tbody>\\n<tr><td style=\"text-align: right;\">  3.0</td><td style=\"text-align: right;\">3.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            3.00</td><td style=\"text-align: right;\">              2.86</td><td>Time Machine, The (2002)</td><td>Adventure</td></tr>\\n<tr><td style=\"text-align: right;\">  2.9</td><td style=\"text-align: right;\">3.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            3.00</td><td style=\"text-align: right;\">              2.86</td><td>Time Machine, The (2002)</td><td>Action   </td></tr>\\n<tr><td style=\"text-align: right;\">  2.8</td><td style=\"text-align: right;\">3.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            3.00</td><td style=\"text-align: right;\">              2.86</td><td>Time Machine, The (2002)</td><td>Sci-Fi   </td></tr>\\n<tr><td style=\"text-align: right;\">  2.0</td><td style=\"text-align: right;\">1.5</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.75</td><td style=\"text-align: right;\">              3.52</td><td>Road to Perdition (2002)</td><td>Crime    </td></tr>\\n<tr><td style=\"text-align: right;\">  2.0</td><td style=\"text-align: right;\">2.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.75</td><td style=\"text-align: right;\">              3.52</td><td>Gangs of New York (2002)</td><td>Crime    </td></tr>\\n<tr><td style=\"text-align: right;\">  1.9</td><td style=\"text-align: right;\">1.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.50</td><td style=\"text-align: right;\">              4.00</td><td>Beautiful Mind, A (2001)</td><td>Drama    </td></tr>\\n<tr><td style=\"text-align: right;\">  1.8</td><td style=\"text-align: right;\">1.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.00</td><td style=\"text-align: right;\">              4.00</td><td>Beautiful Mind, A (2001)</td><td>Romance  </td></tr>\\n<tr><td style=\"text-align: right;\">  1.6</td><td style=\"text-align: right;\">2.0</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.50</td><td style=\"text-align: right;\">              3.52</td><td>Gangs of New York (2002)</td><td>Drama    </td></tr>\\n<tr><td style=\"text-align: right;\">  1.6</td><td style=\"text-align: right;\">1.5</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">            1.50</td><td style=\"text-align: right;\">              3.52</td><td>Road to Perdition (2002)</td><td>Drama    </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uid =  36 \n",
    "# form a set of user vectors. This is the same vector, transformed and repeated.\n",
    "user_vecs, y_vecs = get_user_vecs(uid, scalerUser.inverse_transform(user_train), item_vecs, user_to_genre)\n",
    "\n",
    "# scale the vectors and make predictions for all movies. Return results sorted by rating.\n",
    "sorted_index, sorted_ypu, sorted_items, sorted_user = predict_uservec(user_vecs, item_vecs, model, u_s, i_s, scaler, \n",
    "                                                                      scalerUser, scalerItem, scaledata=scaledata)\n",
    "sorted_y = y_vecs[sorted_index]\n",
    "\n",
    "#print sorted predictions\n",
    "print_existing_user(sorted_ypu, sorted_y.reshape(-1,1), sorted_user, sorted_items, item_features, ivs, uvs, movie_dict, maxcount = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Similar Items\n",
    "The neural network above produces two feature vectors, a user feature vector $v_u$, and a movie feature vector, $v_m$. These are 32 entry vectors whose values are difficult to interpret. However, similar items will have similar vectors. This information can be used to make recommendations. For example, if a user has rated \"Toy Story 3\" highly, one could recommend similar movies by selecting movies with similar movie feature vectors.\n",
    "\n",
    "A similarity measure is the squared distance between the two vectors $ \\mathbf{v_m^{(k)}}$ and $\\mathbf{v_m^{(i)}}$ :\n",
    "$$\\left\\Vert \\mathbf{v_m^{(k)}} - \\mathbf{v_m^{(i)}}  \\right\\Vert^2 = \\sum_{l=1}^{n}(v_{m_l}^{(k)} - v_{m_l}^{(i)})^2\\tag{1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercise 1\n",
    "\n",
    "Write a function to compute the square distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED_FUNCTION: sq_dist\n",
    "# UNQ_C2\n",
    "def sq_dist(a,b):\n",
    "    \"\"\"\n",
    "    Returns the squared distance between two vectors\n",
    "    Args:\n",
    "      a (ndarray (n,)): vector with n features\n",
    "      b (ndarray (n,)): vector with n features\n",
    "    Returns:\n",
    "      d (float) : distance\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###     \n",
    "    d = np.sum(np.square(a-b))\n",
    "    ### END CODE HERE ###     \n",
    "    return (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared distance between a1 and b1: 0.0\n",
      "squared distance between a2 and b2: 0.030000000000000054\n",
      "squared distance between a3 and b3: 2\n"
     ]
    }
   ],
   "source": [
    "a1 = np.array([1.0, 2.0, 3.0]); b1 = np.array([1.0, 2.0, 3.0])\n",
    "a2 = np.array([1.1, 2.1, 3.1]); b2 = np.array([1.0, 2.0, 3.0])\n",
    "a3 = np.array([0, 1, 0]);       b3 = np.array([1, 0, 0])\n",
    "print(f\"squared distance between a1 and b1: {sq_dist(a1, b1)}\")\n",
    "print(f\"squared distance between a2 and b2: {sq_dist(a2, b2)}\")\n",
    "print(f\"squared distance between a3 and b3: {sq_dist(a3, b3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A matrix of distances between movies can be computed once when the model is trained and then reused for new recommendations without retraining. The first step, once a model is trained, is to obtain the movie feature vector, $v_m$, for each of the movies. To do this, we will use the trained `item_NN` and build a small model to allow us to run the movie vectors through it to generate $v_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 16)]              0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 32)                41376     \n",
      "                                                                 \n",
      " tf.math.l2_normalize_2 (TFO  (None, 32)               0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,376\n",
      "Trainable params: 41,376\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_item_m = tf.keras.layers.Input(shape=(num_item_features))    # input layer\n",
    "vm_m = item_NN(input_item_m)                                       # use the trained item_NN\n",
    "vm_m = tf.linalg.l2_normalize(vm_m, axis=1)                        # incorporate normalization as was done in the original model\n",
    "model_m = Model(input_item_m, vm_m)                                \n",
    "model_m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have a movie model, you can create a set of movie feature vectors by using the model to predict using a set of item/movie vectors as input. `item_vecs` is a set of all of the movie vectors. Recall that the same movie will appear as a separate vector for each of its genres. It must be scaled to use with the trained model. The result of the prediction is a 32 entry feature vector for each movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/59 [==============================] - 0s 2ms/step\n",
      "size of all predicted movie feature vectors: (1883, 32)\n"
     ]
    }
   ],
   "source": [
    "scaled_item_vecs = scalerItem.transform(item_vecs)\n",
    "vms = model_m.predict(scaled_item_vecs[:,i_s:])\n",
    "print(f\"size of all predicted movie feature vectors: {vms.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compute a matrix of the squared distance between each movie feature vector and all other movie feature vectors:\n",
    "<figure>\n",
    "    <left> <img src=\"./images/distmatrix.PNG\"   style=\"width:400px;height:225px;\" ></center>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then find the closest movie by finding the minimum along each row. We will make use of [numpy masked arrays](https://numpy.org/doc/1.21/user/tutorial-ma.html) to avoid selecting the same movie. The masked values along the diagonal won't be included in the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>movie1                                </th><th>genres   </th><th>movie2                                             </th><th>genres   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Save the Last Dance (2001)            </td><td>Drama    </td><td>John Q (2002)                                      </td><td>Drama    </td></tr>\n",
       "<tr><td>Save the Last Dance (2001)            </td><td>Romance  </td><td>Saving Silverman (Evil Woman) (2001)               </td><td>Romance  </td></tr>\n",
       "<tr><td>Wedding Planner, The (2001)           </td><td>Comedy   </td><td>National Lampoon&#x27;s Van Wilder (2002)               </td><td>Comedy   </td></tr>\n",
       "<tr><td>Wedding Planner, The (2001)           </td><td>Romance  </td><td>Sweetest Thing, The (2002)                         </td><td>Romance  </td></tr>\n",
       "<tr><td>Hannibal (2001)                       </td><td>Horror   </td><td>Final Destination 2 (2003)                         </td><td>Horror   </td></tr>\n",
       "<tr><td>Hannibal (2001)                       </td><td>Thriller </td><td>Sum of All Fears, The (2002)                       </td><td>Thriller </td></tr>\n",
       "<tr><td>Saving Silverman (Evil Woman) (2001)  </td><td>Comedy   </td><td>Cats &amp; Dogs (2001)                                 </td><td>Comedy   </td></tr>\n",
       "<tr><td>Saving Silverman (Evil Woman) (2001)  </td><td>Romance  </td><td>Mona Lisa Smile (2003)                             </td><td>Romance  </td></tr>\n",
       "<tr><td>Down to Earth (2001)                  </td><td>Comedy   </td><td>Joe Dirt (2001)                                    </td><td>Comedy   </td></tr>\n",
       "<tr><td>Down to Earth (2001)                  </td><td>Fantasy  </td><td>Haunted Mansion, The (2003)                        </td><td>Fantasy  </td></tr>\n",
       "<tr><td>Down to Earth (2001)                  </td><td>Romance  </td><td>Joe Dirt (2001)                                    </td><td>Romance  </td></tr>\n",
       "<tr><td>Mexican, The (2001)                   </td><td>Action   </td><td>Knight&#x27;s Tale, A (2001)                            </td><td>Action   </td></tr>\n",
       "<tr><td>Mexican, The (2001)                   </td><td>Comedy   </td><td>Knight&#x27;s Tale, A (2001)                            </td><td>Comedy   </td></tr>\n",
       "<tr><td>15 Minutes (2001)                     </td><td>Thriller </td><td>Panic Room (2002)                                  </td><td>Thriller </td></tr>\n",
       "<tr><td>Heartbreakers (2001)                  </td><td>Comedy   </td><td>Animal, The (2001)                                 </td><td>Comedy   </td></tr>\n",
       "<tr><td>Heartbreakers (2001)                  </td><td>Crime    </td><td>Stepford Wives, The (2004)                         </td><td>Thriller </td></tr>\n",
       "<tr><td>Heartbreakers (2001)                  </td><td>Romance  </td><td>Bewitched (2005)                                   </td><td>Comedy   </td></tr>\n",
       "<tr><td>Spy Kids (2001)                       </td><td>Action   </td><td>Men in Black II (a.k.a. MIIB) (a.k.a. MIB 2) (2002)</td><td>Action   </td></tr>\n",
       "<tr><td>Spy Kids (2001)                       </td><td>Adventure</td><td>Lara Croft: Tomb Raider (2001)                     </td><td>Adventure</td></tr>\n",
       "<tr><td>Spy Kids (2001)                       </td><td>Children </td><td>Princess Diaries, The (2001)                       </td><td>Children </td></tr>\n",
       "<tr><td>Spy Kids (2001)                       </td><td>Comedy   </td><td>Men in Black II (a.k.a. MIIB) (a.k.a. MIB 2) (2002)</td><td>Comedy   </td></tr>\n",
       "<tr><td>Along Came a Spider (2001)            </td><td>Action   </td><td>Swordfish (2001)                                   </td><td>Action   </td></tr>\n",
       "<tr><td>Along Came a Spider (2001)            </td><td>Crime    </td><td>Swordfish (2001)                                   </td><td>Crime    </td></tr>\n",
       "<tr><td>Along Came a Spider (2001)            </td><td>Mystery  </td><td>Ring, The (2002)                                   </td><td>Mystery  </td></tr>\n",
       "<tr><td>Along Came a Spider (2001)            </td><td>Thriller </td><td>Signs (2002)                                       </td><td>Thriller </td></tr>\n",
       "<tr><td>Blow (2001)                           </td><td>Crime    </td><td>Training Day (2001)                                </td><td>Crime    </td></tr>\n",
       "<tr><td>Blow (2001)                           </td><td>Drama    </td><td>Training Day (2001)                                </td><td>Drama    </td></tr>\n",
       "<tr><td>Bridget Jones&#x27;s Diary (2001)          </td><td>Comedy   </td><td>Super Troopers (2001)                              </td><td>Comedy   </td></tr>\n",
       "<tr><td>Bridget Jones&#x27;s Diary (2001)          </td><td>Drama    </td><td>Others, The (2001)                                 </td><td>Drama    </td></tr>\n",
       "<tr><td>Bridget Jones&#x27;s Diary (2001)          </td><td>Romance  </td><td>Punch-Drunk Love (2002)                            </td><td>Romance  </td></tr>\n",
       "<tr><td>Joe Dirt (2001)                       </td><td>Adventure</td><td>Bulletproof Monk (2003)                            </td><td>Action   </td></tr>\n",
       "<tr><td>Joe Dirt (2001)                       </td><td>Comedy   </td><td>Dr. Dolittle 2 (2001)                              </td><td>Comedy   </td></tr>\n",
       "<tr><td>Joe Dirt (2001)                       </td><td>Mystery  </td><td>Grudge, The (2004)                                 </td><td>Mystery  </td></tr>\n",
       "<tr><td>Joe Dirt (2001)                       </td><td>Romance  </td><td>Down to Earth (2001)                               </td><td>Romance  </td></tr>\n",
       "<tr><td>Crocodile Dundee in Los Angeles (2001)</td><td>Comedy   </td><td>Heartbreakers (2001)                               </td><td>Comedy   </td></tr>\n",
       "<tr><td>Crocodile Dundee in Los Angeles (2001)</td><td>Drama    </td><td>Heartbreakers (2001)                               </td><td>Romance  </td></tr>\n",
       "<tr><td>Mummy Returns, The (2001)             </td><td>Action   </td><td>Swordfish (2001)                                   </td><td>Action   </td></tr>\n",
       "<tr><td>Mummy Returns, The (2001)             </td><td>Adventure</td><td>Terminator 3: Rise of the Machines (2003)          </td><td>Adventure</td></tr>\n",
       "<tr><td>Mummy Returns, The (2001)             </td><td>Comedy   </td><td>American Pie 2 (2001)                              </td><td>Comedy   </td></tr>\n",
       "<tr><td>Mummy Returns, The (2001)             </td><td>Thriller </td><td>Star Trek: Nemesis (2002)                          </td><td>Thriller </td></tr>\n",
       "<tr><td>Knight&#x27;s Tale, A (2001)               </td><td>Action   </td><td>Mexican, The (2001)                                </td><td>Action   </td></tr>\n",
       "<tr><td>Knight&#x27;s Tale, A (2001)               </td><td>Comedy   </td><td>Mexican, The (2001)                                </td><td>Comedy   </td></tr>\n",
       "<tr><td>Knight&#x27;s Tale, A (2001)               </td><td>Romance  </td><td>Bruce Almighty (2003)                              </td><td>Romance  </td></tr>\n",
       "<tr><td>Shrek (2001)                          </td><td>Adventure</td><td>Monsters, Inc. (2001)                              </td><td>Adventure</td></tr>\n",
       "<tr><td>Shrek (2001)                          </td><td>Animation</td><td>Monsters, Inc. (2001)                              </td><td>Animation</td></tr>\n",
       "<tr><td>Shrek (2001)                          </td><td>Children </td><td>Monsters, Inc. (2001)                              </td><td>Children </td></tr>\n",
       "<tr><td>Shrek (2001)                          </td><td>Comedy   </td><td>Monsters, Inc. (2001)                              </td><td>Comedy   </td></tr>\n",
       "<tr><td>Shrek (2001)                          </td><td>Fantasy  </td><td>Monsters, Inc. (2001)                              </td><td>Fantasy  </td></tr>\n",
       "<tr><td>Shrek (2001)                          </td><td>Romance  </td><td>Monsoon Wedding (2001)                             </td><td>Romance  </td></tr>\n",
       "<tr><td>Animal, The (2001)                    </td><td>Comedy   </td><td>Heartbreakers (2001)                               </td><td>Comedy   </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "'<table>\\n<thead>\\n<tr><th>movie1                                </th><th>genres   </th><th>movie2                                             </th><th>genres   </th></tr>\\n</thead>\\n<tbody>\\n<tr><td>Save the Last Dance (2001)            </td><td>Drama    </td><td>John Q (2002)                                      </td><td>Drama    </td></tr>\\n<tr><td>Save the Last Dance (2001)            </td><td>Romance  </td><td>Saving Silverman (Evil Woman) (2001)               </td><td>Romance  </td></tr>\\n<tr><td>Wedding Planner, The (2001)           </td><td>Comedy   </td><td>National Lampoon&#x27;s Van Wilder (2002)               </td><td>Comedy   </td></tr>\\n<tr><td>Wedding Planner, The (2001)           </td><td>Romance  </td><td>Sweetest Thing, The (2002)                         </td><td>Romance  </td></tr>\\n<tr><td>Hannibal (2001)                       </td><td>Horror   </td><td>Final Destination 2 (2003)                         </td><td>Horror   </td></tr>\\n<tr><td>Hannibal (2001)                       </td><td>Thriller </td><td>Sum of All Fears, The (2002)                       </td><td>Thriller </td></tr>\\n<tr><td>Saving Silverman (Evil Woman) (2001)  </td><td>Comedy   </td><td>Cats &amp; Dogs (2001)                                 </td><td>Comedy   </td></tr>\\n<tr><td>Saving Silverman (Evil Woman) (2001)  </td><td>Romance  </td><td>Mona Lisa Smile (2003)                             </td><td>Romance  </td></tr>\\n<tr><td>Down to Earth (2001)                  </td><td>Comedy   </td><td>Joe Dirt (2001)                                    </td><td>Comedy   </td></tr>\\n<tr><td>Down to Earth (2001)                  </td><td>Fantasy  </td><td>Haunted Mansion, The (2003)                        </td><td>Fantasy  </td></tr>\\n<tr><td>Down to Earth (2001)                  </td><td>Romance  </td><td>Joe Dirt (2001)                                    </td><td>Romance  </td></tr>\\n<tr><td>Mexican, The (2001)                   </td><td>Action   </td><td>Knight&#x27;s Tale, A (2001)                            </td><td>Action   </td></tr>\\n<tr><td>Mexican, The (2001)                   </td><td>Comedy   </td><td>Knight&#x27;s Tale, A (2001)                            </td><td>Comedy   </td></tr>\\n<tr><td>15 Minutes (2001)                     </td><td>Thriller </td><td>Panic Room (2002)                                  </td><td>Thriller </td></tr>\\n<tr><td>Heartbreakers (2001)                  </td><td>Comedy   </td><td>Animal, The (2001)                                 </td><td>Comedy   </td></tr>\\n<tr><td>Heartbreakers (2001)                  </td><td>Crime    </td><td>Stepford Wives, The (2004)                         </td><td>Thriller </td></tr>\\n<tr><td>Heartbreakers (2001)                  </td><td>Romance  </td><td>Bewitched (2005)                                   </td><td>Comedy   </td></tr>\\n<tr><td>Spy Kids (2001)                       </td><td>Action   </td><td>Men in Black II (a.k.a. MIIB) (a.k.a. MIB 2) (2002)</td><td>Action   </td></tr>\\n<tr><td>Spy Kids (2001)                       </td><td>Adventure</td><td>Lara Croft: Tomb Raider (2001)                     </td><td>Adventure</td></tr>\\n<tr><td>Spy Kids (2001)                       </td><td>Children </td><td>Princess Diaries, The (2001)                       </td><td>Children </td></tr>\\n<tr><td>Spy Kids (2001)                       </td><td>Comedy   </td><td>Men in Black II (a.k.a. MIIB) (a.k.a. MIB 2) (2002)</td><td>Comedy   </td></tr>\\n<tr><td>Along Came a Spider (2001)            </td><td>Action   </td><td>Swordfish (2001)                                   </td><td>Action   </td></tr>\\n<tr><td>Along Came a Spider (2001)            </td><td>Crime    </td><td>Swordfish (2001)                                   </td><td>Crime    </td></tr>\\n<tr><td>Along Came a Spider (2001)            </td><td>Mystery  </td><td>Ring, The (2002)                                   </td><td>Mystery  </td></tr>\\n<tr><td>Along Came a Spider (2001)            </td><td>Thriller </td><td>Signs (2002)                                       </td><td>Thriller </td></tr>\\n<tr><td>Blow (2001)                           </td><td>Crime    </td><td>Training Day (2001)                                </td><td>Crime    </td></tr>\\n<tr><td>Blow (2001)                           </td><td>Drama    </td><td>Training Day (2001)                                </td><td>Drama    </td></tr>\\n<tr><td>Bridget Jones&#x27;s Diary (2001)          </td><td>Comedy   </td><td>Super Troopers (2001)                              </td><td>Comedy   </td></tr>\\n<tr><td>Bridget Jones&#x27;s Diary (2001)          </td><td>Drama    </td><td>Others, The (2001)                                 </td><td>Drama    </td></tr>\\n<tr><td>Bridget Jones&#x27;s Diary (2001)          </td><td>Romance  </td><td>Punch-Drunk Love (2002)                            </td><td>Romance  </td></tr>\\n<tr><td>Joe Dirt (2001)                       </td><td>Adventure</td><td>Bulletproof Monk (2003)                            </td><td>Action   </td></tr>\\n<tr><td>Joe Dirt (2001)                       </td><td>Comedy   </td><td>Dr. Dolittle 2 (2001)                              </td><td>Comedy   </td></tr>\\n<tr><td>Joe Dirt (2001)                       </td><td>Mystery  </td><td>Grudge, The (2004)                                 </td><td>Mystery  </td></tr>\\n<tr><td>Joe Dirt (2001)                       </td><td>Romance  </td><td>Down to Earth (2001)                               </td><td>Romance  </td></tr>\\n<tr><td>Crocodile Dundee in Los Angeles (2001)</td><td>Comedy   </td><td>Heartbreakers (2001)                               </td><td>Comedy   </td></tr>\\n<tr><td>Crocodile Dundee in Los Angeles (2001)</td><td>Drama    </td><td>Heartbreakers (2001)                               </td><td>Romance  </td></tr>\\n<tr><td>Mummy Returns, The (2001)             </td><td>Action   </td><td>Swordfish (2001)                                   </td><td>Action   </td></tr>\\n<tr><td>Mummy Returns, The (2001)             </td><td>Adventure</td><td>Terminator 3: Rise of the Machines (2003)          </td><td>Adventure</td></tr>\\n<tr><td>Mummy Returns, The (2001)             </td><td>Comedy   </td><td>American Pie 2 (2001)                              </td><td>Comedy   </td></tr>\\n<tr><td>Mummy Returns, The (2001)             </td><td>Thriller </td><td>Star Trek: Nemesis (2002)                          </td><td>Thriller </td></tr>\\n<tr><td>Knight&#x27;s Tale, A (2001)               </td><td>Action   </td><td>Mexican, The (2001)                                </td><td>Action   </td></tr>\\n<tr><td>Knight&#x27;s Tale, A (2001)               </td><td>Comedy   </td><td>Mexican, The (2001)                                </td><td>Comedy   </td></tr>\\n<tr><td>Knight&#x27;s Tale, A (2001)               </td><td>Romance  </td><td>Bruce Almighty (2003)                              </td><td>Romance  </td></tr>\\n<tr><td>Shrek (2001)                          </td><td>Adventure</td><td>Monsters, Inc. (2001)                              </td><td>Adventure</td></tr>\\n<tr><td>Shrek (2001)                          </td><td>Animation</td><td>Monsters, Inc. (2001)                              </td><td>Animation</td></tr>\\n<tr><td>Shrek (2001)                          </td><td>Children </td><td>Monsters, Inc. (2001)                              </td><td>Children </td></tr>\\n<tr><td>Shrek (2001)                          </td><td>Comedy   </td><td>Monsters, Inc. (2001)                              </td><td>Comedy   </td></tr>\\n<tr><td>Shrek (2001)                          </td><td>Fantasy  </td><td>Monsters, Inc. (2001)                              </td><td>Fantasy  </td></tr>\\n<tr><td>Shrek (2001)                          </td><td>Romance  </td><td>Monsoon Wedding (2001)                             </td><td>Romance  </td></tr>\\n<tr><td>Animal, The (2001)                    </td><td>Comedy   </td><td>Heartbreakers (2001)                               </td><td>Comedy   </td></tr>\\n</tbody>\\n</table>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 50\n",
    "dim = len(vms)\n",
    "dist = np.zeros((dim,dim))\n",
    "\n",
    "for i in range(dim):\n",
    "    for j in range(dim):\n",
    "        dist[i,j] = sq_dist(vms[i, :], vms[j, :])\n",
    "        \n",
    "m_dist = ma.masked_array(dist, mask=np.identity(dist.shape[0]))  # mask the diagonal\n",
    "\n",
    "disp = [[\"movie1\", \"genres\", \"movie2\", \"genres\"]]\n",
    "for i in range(count):\n",
    "    min_idx = np.argmin(m_dist[i])\n",
    "    movie1_id = int(item_vecs[i,0])\n",
    "    movie2_id = int(item_vecs[min_idx,0])\n",
    "    genre1,_  = get_item_genre(item_vecs[i,:], ivs, item_features)\n",
    "    genre2,_  = get_item_genre(item_vecs[min_idx,:], ivs, item_features)\n",
    "\n",
    "    disp.append( [movie_dict[movie1_id]['title'], genre1,\n",
    "                movie_dict[movie2_id]['title'], genre2]\n",
    "    )\n",
    "table = tabulate.tabulate(disp, tablefmt='html', headers=\"firstrow\", floatfmt=[\".1f\", \".1f\", \".0f\", \".2f\", \".2f\"])\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results show the model will suggest a movie from the same genre."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOFYdA6zQJ1FpgYwYmRIeXa",
   "collapsed_sections": [],
   "name": "Recsys_NN.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1RO0HLb7kRE0Tj_0D4E5I-vQz2QLu3CUm",
     "timestamp": 1655169179306
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
